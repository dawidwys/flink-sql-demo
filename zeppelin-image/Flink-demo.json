{"paragraphs":[{"text":"%flink.conf\n\nFLINK_HOME /zeppelin/flink-dist\nzeppelin.flink.planner blink\nexecution.target remote\nexecution.attached true\nflink.execution.mode remote\nflink.execution.remote.host jobmanager\nflink.execution.remote.port 8081\nrest.address jobmanager\nrest.port 8081\njobmanager.rpc.address jobmanager","user":"anonymous","config":{"editorSetting":{"language":"text","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/text","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1575969692776_1889016811","id":"paragraph_1575969692776_1889016811","dateCreated":"2020-04-21T11:21:57+0000","status":"FINISHED","focus":true,"$$hashKey":"object:307","runtimeInfos":{}},{"title":"Create hive catalog","text":"%flink\n\nimport org.apache.flink.table.catalog.hive.HiveCatalog\nval hiveCatalog = new HiveCatalog(\"hive\", \"default\", \"/zeppelin/flink-dist/conf\", \"2.3.6\")\nstenv.registerCatalog(\"hive\", hiveCatalog)\nstenv.useCatalog(\"hive\")\n","user":"anonymous","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":false,"runOnSelectionChange":true,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.flink.table.catalog.hive.HiveCatalog\n\u001b[1m\u001b[34mhiveCatalog\u001b[0m: \u001b[1m\u001b[32morg.apache.flink.table.catalog.hive.HiveCatalog\u001b[0m = org.apache.flink.table.catalog.hive.HiveCatalog@ce70f46\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1575969864322_1868173907","id":"paragraph_1575969864322_1868173907","dateCreated":"2020-04-21T11:22:11+0000","status":"FINISHED","$$hashKey":"object:308","runtimeInfos":{}},{"text":"%flink\nstenv.useCatalog(\"hive\")","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111823_-366814411","id":"paragraph_1587465527985_1829362776","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:309"},{"text":"%flink.ssql\nSHOW TABLES;\n","user":"anonymous","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"table":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"table\norders\nprod_customer\nprod_nation\nprod_rates\nprod_region\ntest\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587465391858_1400052844","id":"paragraph_1587465391858_1400052844","dateCreated":"2020-04-21T11:22:16+0000","status":"FINISHED","$$hashKey":"object:310","runtimeInfos":{}},{"text":"%flink\nstenv.sqlUpdate(\"CREATE TABLE Orders2 (\" +\n  \"O_ORDERKEY INTEGER,\" +\n  \"O_CUSTKEY INTEGER,\" +\n  \"O_ORDERSTATUS STRING,\" +\n  \"O_TOTALPRICE DECIMAL(38,18),\" +\n  \"O_ORDERDATE DATE,\" +\n  \"O_ORDERPRIORITY STRING,\" +\n  \"O_CLERK STRING,\" +\n  \"O_SHIPPRIORITY INTEGER,\" +\n  \"O_COMMENT STRING,\" +\n  \"PROC_TIME AS PROCTIME()\" +\n\") WITH (\" +\n  \"'connector.type' = 'kafka',\" +\n  \"'connector.version' = 'universal',\" +\n  \"'connector.topic' = 'Orders',\" +\n  \"'connector.properties.zookeeper.connect' = 'not-needed',\" +\n  \"'connector.properties.bootstrap.servers' = 'kafka:9092',\" +\n  \"'connector.startup-mode' = 'earliest-offset',\" +\n  \"'format.type' = 'csv',\" +\n  \"'format.field-delimiter' = '|'\" +\n\")\");","user":"anonymous","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587465959005_1245544991","id":"paragraph_1587465959005_1245544991","dateCreated":"2020-04-21T11:24:15+0000","status":"FINISHED","$$hashKey":"object:311","runtimeInfos":{}},{"text":"%flink.ssql\nDESCRIBE orders;","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Column\tType\nOptional[O_ORDERKEY]\tOptional[INT]\nOptional[O_CUSTKEY]\tOptional[INT]\nOptional[O_ORDERSTATUS]\tOptional[CHAR(1)]\nOptional[O_TOTALPRICE]\tOptional[DECIMAL(15, 2)]\nOptional[O_ORDERDATE]\tOptional[DATE]\nOptional[O_ORDERPRIORITY]\tOptional[CHAR(15)]\nOptional[O_CLERK]\tOptional[CHAR(15)]\nOptional[O_SHIPPRIORITY]\tOptional[INT]\nOptional[O_COMMENT]\tOptional[VARCHAR(79)]\nOptional[PROC_TIME]\tOptional[TIMESTAMP(3) NOT NULL]\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111824_-1156869377","id":"paragraph_1587467538425_-2081061850","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:312"},{"text":"%flink.ssql(type=retract, refreshInterval=2000, parallelism=1, enableSavePoint=false,  runWithSavePoint=false)\nSELECT O_ORDERKEY FROM Orders2;","user":"anonymous","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"O_ORDERKEY":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TABLE","data":"O_ORDERKEY\n"},{"type":"TEXT","data":"Fail to run sql command: SELECT O_ORDERKEY FROM Orders2\njava.io.IOException: Fail to run stream sql job\n\tat org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:159)\n\tat org.apache.zeppelin.flink.FlinkStreamSqlInterpreter.callSelect(FlinkStreamSqlInterpreter.java:75)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.callCommand(FlinkSqlInterrpeter.java:158)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.runSqlList(FlinkSqlInterrpeter.java:122)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.interpret(FlinkSqlInterrpeter.java:80)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:677)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:570)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:121)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:39)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: d5dd1a7fd6ac4b11795e0298198fcc95)\n\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)\n\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)\n\tat org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1640)\n\tat org.apache.flink.api.java.ScalaShellStreamEnvironment.execute(ScalaShellStreamEnvironment.java:80)\n\tat org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1620)\n\tat org.apache.flink.table.planner.delegation.StreamExecutor.execute(StreamExecutor.java:42)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:643)\n\tat org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:148)\n\t... 13 more\nCaused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: d5dd1a7fd6ac4b11795e0298198fcc95)\n\tat org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:112)\n\tat java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\n\tat java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\n\tat java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)\n\tat org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$21(RestClusterClient.java:565)\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\n\tat java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)\n\tat org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:291)\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\n\tat java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)\n\tat java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)\n\tat java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\n\t... 3 more\nCaused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.\n\tat org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)\n\tat org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:110)\n\t... 19 more\nCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:186)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:180)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:484)\n\tat org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)\n\tat org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)\n\tat akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)\n\tat akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)\n\tat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)\n\tat akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:517)\n\tat akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:561)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:225)\n\tat akka.dispatch.Mailbox.exec(Mailbox.scala:235)\n\tat akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\nCaused by: java.io.IOException: Failed to deserialize CSV row 'a'.\n\tat org.apache.flink.formats.csv.CsvRowDeserializationSchema.deserialize(CsvRowDeserializationSchema.java:166)\n\tat org.apache.flink.formats.csv.CsvRowDeserializationSchema.deserialize(CsvRowDeserializationSchema.java:56)\n\tat org.apache.flink.streaming.connectors.kafka.internals.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:45)\n\tat org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:140)\n\tat org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:715)\n\tat org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)\n\tat org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)\n\tat org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:196)\nCaused by: java.lang.RuntimeException: Row length mismatch. 9 fields expected but was 1.\n\tat org.apache.flink.formats.csv.CsvRowDeserializationSchema.validateArity(CsvRowDeserializationSchema.java:370)\n\tat org.apache.flink.formats.csv.CsvRowDeserializationSchema.lambda$assembleRowRuntimeConverter$ddbf7d3b$1(CsvRowDeserializationSchema.java:251)\n\tat org.apache.flink.formats.csv.CsvRowDeserializationSchema.deserialize(CsvRowDeserializationSchema.java:161)\n\t... 7 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587465596186_-1687026728","id":"paragraph_1587465596186_-1687026728","dateCreated":"2020-04-21T11:25:08+0000","status":"ERROR","$$hashKey":"object:313","runtimeInfos":{}},{"title":"Create csv table","text":"%flink\nstenv.sqlUpdate(\"CREATE TABLE `CsvTable3`(name string, id bigint, eventTime TIMESTAMP(3), WATERMARK FOR eventTime AS eventTime - INTERVAL '1' SECOND) with ('connector.type' = 'filesystem', 'connector.path' = 'file:///tmp/test.csv', 'format.type' = 'csv')\")","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111824_1650668389","id":"paragraph_1576436852200_958499824","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:314"},{"text":"%flink\nstenv.sqlQuery(\"SELECT TUMBLE_START(eventTime, INTERVAL '1' SECOND) as `start`, TUMBLE_END(eventTime, INTERVAL '1' SECOND) as `end`, count(*) as `count` FROM `CsvTable2` GROUP BY TUMBLE(eventTime, INTERVAL '1' SECOND)\").toAppendStream[Row].print\nstenv.execute(\"Job\")","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.flink.table.api.TableException: Rowtime extraction expression missing. Please report a bug.\n  at org.apache.flink.table.planner.codegen.ExprCodeGenerator$$anonfun$1.apply(ExprCodeGenerator.scala:163)\n  at org.apache.flink.table.planner.codegen.ExprCodeGenerator$$anonfun$1.apply(ExprCodeGenerator.scala:156)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.mutable.ArrayOps$ofInt.map(ArrayOps.scala:234)\n  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateConverterResultExpression(ExprCodeGenerator.scala:156)\n  at org.apache.flink.table.planner.plan.utils.ScanUtil$.convertToInternalRow(ScanUtil.scala:102)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.translateToPlanInternal(StreamExecTableSourceScan.scala:149)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.translateToPlanInternal(StreamExecTableSourceScan.scala:62)\n  at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.translateToPlan(StreamExecTableSourceScan.scala:62)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWatermarkAssigner.translateToPlanInternal(StreamExecWatermarkAssigner.scala:103)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWatermarkAssigner.translateToPlanInternal(StreamExecWatermarkAssigner.scala:46)\n  at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWatermarkAssigner.translateToPlan(StreamExecWatermarkAssigner.scala:46)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:54)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39)\n  at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange.translateToPlanInternal(StreamExecExchange.scala:84)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange.translateToPlanInternal(StreamExecExchange.scala:44)\n  at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange.translateToPlan(StreamExecExchange.scala:44)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.translateToPlanInternal(StreamExecGroupWindowAggregateBase.scala:125)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.translateToPlanInternal(StreamExecGroupWindowAggregateBase.scala:54)\n  at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.translateToPlan(StreamExecGroupWindowAggregateBase.scala:54)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:54)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39)\n  at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToTransformation(StreamExecSink.scala:185)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:154)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:50)\n  at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)\n  at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:50)\n  at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)\n  at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:59)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n  at scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n  at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:59)\n  at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:152)\n  at org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.scala:210)\n  at org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.scala:107)\n  at org.apache.flink.table.api.scala.TableConversions.toAppendStream(TableConversions.scala:101)\n  ... 58 elided\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111824_308036808","id":"paragraph_1576483630958_1314349517","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:315"},{"text":"%flink\nstenv.sqlQuery(\"SELECT * FROM `CsvTable2`\").getSchema.toString","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mres56\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m =\n\"root\n |-- name: STRING\n |-- id: BIGINT\n |-- eventTime: TIMESTAMP(3) *ROWTIME*\n\"\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111825_2115672149","id":"paragraph_1576484288960_-1115204156","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:316"},{"title":"Create kafka table","text":"%flink\nstenv.sqlUpdate(\n\"\"\"CREATE TABLE `hive`.`default`.`KafkaCsvTable`(\n  name string,\n  id bigint) \nwith (\n'connector.type' = 'kafka',       \n'connector.version' = 'universal', \n'connector.topic' = 'test_topic',\n'update-mode' = 'append',\n'connector.properties.zookeeper.connect' = 'zookeeper:2181', -- required: specify the ZooKeeper connection string\n'connector.properties.bootstrap.servers' = 'kafka:9092', -- required: specify the Kafka server connection string\n'connector.startup-mode' = 'earliest-offset', \n'format.type' = 'csv'\n)\n\"\"\")","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.flink.table.api.ValidationException: Could not execute CreateTable in path `hive`.`default`.`KafkaCsvTable`\n  at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:568)\n  at org.apache.flink.table.catalog.CatalogManager.createTable(CatalogManager.java:451)\n  at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:499)\n  ... 46 elided\nCaused by: org.apache.flink.table.catalog.exceptions.TableAlreadyExistException: Table (or view) default.KafkaCsvTable already exists in Catalog hive.\n  at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:360)\n  at org.apache.flink.table.catalog.CatalogManager.lambda$createTable$9(CatalogManager.java:452)\n  at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:566)\n  ... 48 more\nCaused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table KafkaCsvTable already exists\n  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42052)\n  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42038)\n  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:41964)\n  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)\n  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1199)\n  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1185)\n  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2399)\n  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:752)\n  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:740)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:169)\n  at com.sun.proxy.$Proxy30.createTable(Unknown Source)\n  at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createTable(HiveMetastoreClientWrapper.java:142)\n  at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:357)\n  ... 50 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111825_249297233","id":"paragraph_1576436882957_-2110178065","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:317"},{"text":"%flink\nstenv.useCatalog(\"hive\")\nstenv.sqlUpdate(\"INSERT INTO `KafkaCsvTable` SELECT * FROM `CsvTable`\")\nstenv.execute(\"Job\")","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mres17\u001b[0m: \u001b[1m\u001b[32morg.apache.flink.api.common.JobExecutionResult\u001b[0m = org.apache.flink.api.common.JobExecutionResult@4933679\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111825_417758370","id":"paragraph_1576437604183_1321136691","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:318"},{"text":"%flink\nimport org.apache.flink.configuration.PipelineOptions\nimport java.time.Duration\n\ntype JBoolean = java.lang.Boolean\n\nstenv.getConfig.getConfiguration().set[JBoolean](PipelineOptions.AUTO_GENERATE_UIDS, true)\nstenv.getConfig.getConfiguration().set(PipelineOptions.AUTO_WATERMARK_INTERVAL, Duration.ofMillis(350))\nstenv.sqlUpdate(\"INSERT INTO `KafkaCsvTable` SELECT * FROM `KafkaCsvTable`\")\nstenv.execute(\"Job\")","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.apache.flink.configuration.PipelineOptions\nimport java.time.Duration\n\u001b[1m\u001b[34mres27\u001b[0m: \u001b[1m\u001b[32morg.apache.flink.configuration.Configuration\u001b[0m = {pipeline.auto-generate-uids=true}\n\u001b[1m\u001b[34mres28\u001b[0m: \u001b[1m\u001b[32morg.apache.flink.configuration.Configuration\u001b[0m = {pipeline.auto-watermark-interval=PT0.35S, pipeline.auto-generate-uids=true}\norg.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 459f4d89703306e6edcd21af6e6a3384)\n  at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:112)\n  at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\n  at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\n  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\n  at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)\n  at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$21(RestClusterClient.java:532)\n  at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)\n  at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)\n  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\n  at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)\n  at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:291)\n  at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)\n  at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)\n  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\n  at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)\n  at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)\n  at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n  at java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled.\n  at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:148)\n  at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:110)\n  ... 19 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111826_-1168469528","id":"paragraph_1576437691558_-1266836002","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:319"},{"text":"%flink\nstenv.from(\"`hive`.`default`.`TestView`\").toAppendStream[Row].print\nstenv.execute(\"Job\")","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mres6\u001b[0m: \u001b[1m\u001b[32morg.apache.flink.streaming.api.datastream.DataStreamSink[org.apache.flink.types.Row]\u001b[0m = org.apache.flink.streaming.api.datastream.DataStreamSink@3f1cdcda\n\u001b[1m\u001b[34mres7\u001b[0m: \u001b[1m\u001b[32morg.apache.flink.api.common.JobExecutionResult\u001b[0m = org.apache.flink.api.common.JobExecutionResult@665f1842\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111826_1062821961","id":"paragraph_1576437925156_-1333524516","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:320"},{"text":"%flink\nimport org.apache.flink.table.descriptors._\nimport org.apache.flink.table.api.DataTypes\n\nstenv.connect(new Kafka()\n\t\t\t\t\t.version(\"universal\")\n\t\t\t\t\t.property(\"zookeeper.connect\", \"zookeeper:2181\")\n\t\t\t\t\t.property(\"bootstrap.servers\", \"kafka:9092\")\n\t\t\t\t\t.topic(\"fruits\"))\n\t\t\t\t.withFormat(new Csv())\n\t\t\t\t.withSchema(\n\t\t\t\t\tnew Schema()\n\t\t\t\t\t\t.field(\"name\", DataTypes.STRING())\n\t\t\t\t\t\t.field(\"count\", DataTypes.BIGINT()) // no from so it must match with the input\n\t\t\t\t\t\t.field(\"timestamp\", DataTypes.TIMESTAMP(3)).rowtime(new Rowtime().timestampsFromField(\"eventTime\").watermarksPeriodicAscending())\n\t\t\t\t\t\t.field(\"proctime\", DataTypes.TIMESTAMP(3)).proctime())\n\t\t\t\t.inAppendMode()\n\t\t\t\t.createTemporaryTable(\"KafkaFruits3\")\n\t\t\t\t","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.flink.table.descriptors._\nimport org.apache.flink.table.api.DataTypes\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111826_1622192387","id":"paragraph_1576481522607_-527325171","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:321"},{"text":"%flink\nstenv.sqlQuery(\"SELECT * FROM `KafkaFruits3`\").toAppendStream[Row].print\nstenv.execute(\"Job\")","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\u001b[1m\u001b[34mres83\u001b[0m: \u001b[1m\u001b[32morg.apache.flink.streaming.api.datastream.DataStreamSink[org.apache.flink.types.Row]\u001b[0m = org.apache.flink.streaming.api.datastream.DataStreamSink@2954ce7b\norg.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: abdf7a689bae16878702b381d1fc761d)\n  at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:112)\n  at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\n  at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\n  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\n  at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)\n  at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$21(RestClusterClient.java:532)\n  at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)\n  at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)\n  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\n  at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)\n  at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:291)\n  at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)\n  at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)\n  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\n  at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)\n  at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)\n  at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n  at java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled.\n  at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:148)\n  at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:110)\n  ... 19 more\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111826_2130670994","id":"paragraph_1576483193804_1066449545","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:322"},{"text":"%flink\nimport org.apache.flink.table.module.hive.HiveModule\n//stenv.loadModule(\"hive\", new HiveModule(\"2.3.2\"))\n\n// stenv.sqlQuery(\"SELECT factorial(id) FROM (VALUES (1), (2), (3), (4)) AS NameTable(id)\").toAppendStream[Row].print\n// stenv.sqlQuery(\"SELECT to_utc_timestamp(id, 'PST') FROM (VALUES (1), (2), (3), (4)) AS NameTable(id)\").toAppendStream[Row].print\nstenv.sqlQuery(\"SELECT encode(name, encoding) FROM (VALUES ('ABC', 'UTF-8'), ('D', 'UTF-8'), ('F', 'UTF-8')) AS NameTable(name, encoding)\").toAppendStream[Row].print\nstenv.execute(\"Job\")","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.flink.table.module.hive.HiveModule\n\u001b[1m\u001b[34mres111\u001b[0m: \u001b[1m\u001b[32morg.apache.flink.streaming.api.datastream.DataStreamSink[org.apache.flink.types.Row]\u001b[0m = org.apache.flink.streaming.api.datastream.DataStreamSink@78dcb19c\n\u001b[1m\u001b[34mres112\u001b[0m: \u001b[1m\u001b[32morg.apache.flink.api.common.JobExecutionResult\u001b[0m = org.apache.flink.api.common.JobExecutionResult@354c5a27\n"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111827_-490944907","id":"paragraph_1576484640118_-153812302","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:323"},{"text":"%flink\n","user":"anonymous","dateUpdated":"2020-04-21T11:21:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1587468111827_-923136035","id":"paragraph_1576485469808_-1187187609","dateCreated":"2020-04-21T11:21:51+0000","status":"READY","$$hashKey":"object:324"}],"name":"Flink-demo","id":"2F6UHYRXN","defaultInterpreterGroup":"flink","version":"0.9.0-SNAPSHOT","permissions":{},"noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{},"path":"/Flink-demo"}